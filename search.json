[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Marco’s Blog",
    "section": "",
    "text": "Email Automation\n\n\n\nEmail Automation\n\nAI Assistant\n\n\n\nThis Blog is about How I used ChatGPT to help me write an onail to 100 different people\n\n\n\n\n\nSep 17, 2025\n\n\nMarco Firmender\n\n\n\n\n\n\n\n\n\n\n\n\nColor Matching Game\n\n\n\nPhoto and Color Recognition\n\nPhoto Generation\n\n\n\nThis blog is about a game I played with GPT where I gave it an image and the objective to match the colors.\n\n\n\n\n\nSep 14, 2025\n\n\nMarco Firmender\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Sentence Generation\n\n\n\nLLM\n\nReasonable Continuation\n\n\n\nFor this blog I asked chat GPT to generate a sentence using random words and then asked GPT follow up questions.\n\n\n\n\n\nSep 11, 2025\n\n\nMarco Firmender\n\n\n\n\n\n\n\n\n\n\n\n\nDescription Game\n\n\n\nAI image generation\n\nPrompt Engineering\n\n\n\nThis blog is about a game I played with GPT 5.0, where I gave two very vivid prompts to GPT and had it return me an image as accurately and in depth as possible. I then gave it one vague description and compared the differences.\n\n\n\n\n\nSep 10, 2025\n\n\nMarco Firmender\n\n\n\n\n\n\n\n\n\n\n\n\nExploring JupyterLab and GitHub\n\n\n\nJupyterLab\n\nGitHub\n\nLLMs\n\n\n\nThis Blog is about my exploration with GitHub and JupyterLab for the first time, As well as what I have come to learn about LLMs\n\n\n\n\n\nSep 3, 2025\n\n\nMarco Firmender\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Profile_Photo.html",
    "href": "Profile_Photo.html",
    "title": "Marco's Blogs",
    "section": "",
    "text": "Marco Firmender"
  },
  {
    "objectID": "posts/003_BLOGPOST3/003_Blog_post.html",
    "href": "posts/003_BLOGPOST3/003_Blog_post.html",
    "title": "Random Sentence Generation",
    "section": "",
    "text": "For this experiment I wanted to test the strenght of GPT’s reasonable continuation by asking it to generate completely a random sentance. I then judged the human like nature of the sentances I asked it to generate.\n\nI started by giving GPT the Prompt, “generate a sentence for me with words picked completely at random.”\nGPT returned me: “Pineapple lanterns whispered briskly as umbrellas galloped sideways.”\nI then asked it to explain how it chose those words.\nGPT returned me: \nGPT clearly demostrated that it can understand basic sentance structure, and seemd to have chosen random words at first. Because to me the sentance retured seemed more improbable in a conversational sense than random, but could deffinitly make sense in a fictional book or somthing creative. So I wanted to test GPT and make it second guess the randomness of the words it chose, to compare human knowledge to AI.\nGPT Returned me: \nDemonstrating that GPT does understand random word selection in it’s calculation of the next best word for the sentance.\nI then asked GPT to make a list of these sentances:\n\nThe endless equation jumps quickly before the endless equation calculates.\nThe curved algorithm calculates awkwardly although the silent whale dances.\nThe fractured cloud melts loudly because the endless algorithm melts.\nThe odd lantern whispers boldly although the golden saxophone dances.\nThe brisk umbrella whispers quickly while the curved cloud whispers.\nThe odd cloud melts softly after the fractured whale whispers.\nThe brisk saxophone spins quickly as the sideways lantern balances.\nThe golden equation melts awkwardly until the curved saxophone spins.\nThe brisk umbrella whispers boldly after the curved lantern dances.\nThe endless pineapple whispers quickly while the golden umbrella whispers.\n\nwhile these are all random in word choice for each sentance, GPT does seem to use the same words over and over to generate these sentances. Testing the frequency of words chosen by GPT was not the goal of my exeriment but was an interesting secondary finding as it was very high.\nOverall I think this study shows the strength of the GPT as an LLM as it is remarkably good at deciphering radomness as opposed to the next best fit for the sentance."
  },
  {
    "objectID": "posts/005_BLOGPOST5/OO5_BLOG_POST.html",
    "href": "posts/005_BLOGPOST5/OO5_BLOG_POST.html",
    "title": "Email Automation",
    "section": "",
    "text": "Today I played around with ChatGPTs ability to do simple but intricite tasks. I wanted to test the LLMs ability to do a long drawn out task without getting off task or becoming dilusional. So I started by giving ChatGPT an email outline and list of 100 randomly generated names. I then asked GPT to address the email to the 100 names and return me the list so I could copy and paste each one into an email. GPT was remarkably good at this, it returned me the first 10 names and then asked if I wanted the entire list converted into a CSV file, and Excel file, or PDF to save me from scrolling through the the entire 100 names. I first tried the CSV file. I used Google Colab to open the CSV file and filter the CSV for 100 unique names and saw that GPT had done what I asked it perfectly and did not miss a name.\nHere is a screenshot of the CSV file:\nI then wanted to check GPTs ability to do the same task with less instruction so I asked GPT to make a unique email and address them to the same 100 people. The original email I had given GPT was a networking email, my hope was that GPT would pick up on the context, and goal of the email and recreate this in a unique way for each person.\nGPT created 100 emails that at a glance seemed individually unique, but after reading through them GPT seems to have created a bucket of emails and used the same 10 or so emails over again with slight alterations to the grammer, syntax and word choice.\nThis is what GPT returned me:\nI then asked GPT how it generated the random emails and this is what it returned me: \nmy suspicion was correct, GPT then went on to explain how it created variations and used a mixing and matching algorithm to create the emails. GPT created 5 variations for the emails and rotated the Subject, Address, and body of the emails between these variations. This proves that GPT is good at the automation but not great at creating entirely unique emails because the LLM runs on algorithms not human creativity and touch. While GPT may be able to complete the task, it can not replace human creativity."
  },
  {
    "objectID": "posts/004_BLOGPOST4/004_Blog_post.html",
    "href": "posts/004_BLOGPOST4/004_Blog_post.html",
    "title": "Color Matching Game",
    "section": "",
    "text": "For this experiment, I gave ChatGPT-5 an image and asked it to match colors. What I found is that AI is not good at recognizing colors and recreating the image.\nThis was the first prompt I gave GPT:\n\nAnd this is what ChatGPT-5 returned to me:\n\nGPT was both wrong on the colors it matched and wrongly edited the number of notebooks and pens to be matched. It was fascinating to find that GPT was not good at recognizing colors and recreating them, but I wasn’t totally sure if it was a flaw of the LLM or a one-off mistake. Therefore, I asked GPT to do it again. This time, I gave it an ice cream cone with four different color combinations and asked it to recreate the top left combination.\nThis is the image I gave to GPT:\n\nAnd this is what GPT returned to me:\n\nI then asked GPT to explain to me in words what it did. Here is what GPT returned to me:\n\nIt seems that GPT had a big problem differentiating between red, green, orange, and purple. GPT, however, did well with darker colors like brown and dark blue. It was also able to recognize yellow well. I then told GPT about the mistakes it had made and asked it how it recognized the colors and recreated the cone.\nThis is what GPT returned to me:\n\nIt seems that when GPT is scanning an image, it moves quickly and is bad at picking up all the details. GPT communicated that it had problems differentiating the background from the colors on the ice cream, which was interesting to find out. Overall, it was interesting to find out that GPT is bad with color and image recognition and I want to ask Professor about this in class this week."
  },
  {
    "objectID": "posts/002_BLOGPOST2/002_blog_post.html",
    "href": "posts/002_BLOGPOST2/002_blog_post.html",
    "title": "Description Game",
    "section": "",
    "text": "Trial 1 I first tried to play this Game with Anthropic/Claude 3.5 but claude does not have image generation abilities and I did not know that. I then switched to ChatGPT 5.0.\nDescription given to GPT: For this first image, I would like you to generate an Instagram post-sized (1080p x 1080p) picture of myself flying through the sky on a sunny day with a few clouds, on a Griffin from Greek mythology with a lightning bolt in my right hand, and a monkey on my left shoulder. I am a 6-foot, Caucasian male with brown eyes and light brown hair. On my face, I have defined features and a low-profile nose. I want to be wearing a Toga and sandals in this image. I am 215 pounds, but have little body fat (about 10%). Try to recreate my legs and torso as best as you can, given my height and weight.\nCommentary: first GPT asked me to provide a picture of myself to help nail the proportion, I did not provide it as the point of this game was to generate the image purely off of description. I am trying to test the LLMs ability to take my text and return me an image based on the data is has been trained on. The outcome was pretty solid, GPT returned me an image with all the aesthetic details I had asked for but was off a bit on my face. The image came out looking like it was painted which was not my intention but a result of my own prompt. overall I am impressed with GPTs ability to generate the image in the way I had described it.\nOutcome: \nTrial 2\nDescription given to GPT: For this next image, I would like you to create a landscape image depicting a man in traditional Japanese clothing, poling a small boat across Lake Kawaguchiko with Mt. Fuji in the background. I would like it to be a snowy, but sunny day this time. On the left and right edges of the image, I would like you to place some cherry blossoms, half cut off by the frame of the image, and not detracting from the focal point of the image, which is the man in the boat. On the left side, I would like a cherry blossom to be falling off the tree. Orient the man so that he is moving from the left of the image to the right on a slight angle away from the viewpoint of the image. His back should be slightly turned to the viewer of the image. make the image slighty ominous, and the man should be mysterious.\nCommentary: Overall, GPT did a great job capturing the details of the prompt I gave it. It hit on all the significant points I gave it to include and missed nothing.\nOutcome: \nTrial 3\nDescription given to GPT: For this last image, I would like you to recreate this picture, which is the cover photo for the movie The Godfather I would like you to change the main character to a baby, and make the supporting characters in the image a mixture of baby animals. I would also like you to change the color scheme and make sure the medium is as true to the one i gave you as possibe, make it look like photo.\nCommentary: At first, GPT returned me this: I can’t create that specific image since it would involve altering and recreating copyrighted material too directly. But here’s what I can do: I can generate a parody-style original composition inspired by the aesthetic of a classic mafia film poster. It could feature:\nI expected GPT not to be able to produce the photo style medium that the movie poster I gave it had, but it did a remarkably good job. Additionally, I initially gave it a poster to model the redition; however, I have to say the GPT did well recreating the style outside of changing the color scheme. I thought that GPT would have struggled with this element.\nOutcome: \nOverall I am really happy with the outcome of the test, and really shocked at the recent upgrades to the image generation skills of GPT 5.0. In the past I have created images using 4.0 and GPT often struggled with lettering, or recreating the medium I intended. I was also pleasently shocked at its ability to fill in the gaps with my vague description."
  },
  {
    "objectID": "posts/001_BLOGPOST1/001_blog_post.html",
    "href": "posts/001_BLOGPOST1/001_blog_post.html",
    "title": "Exploring JupyterLab and GitHub",
    "section": "",
    "text": "JupyterLab\nAt First, I thought that we were going to be using Jupyter Notebook, so I downloaded Jupyter Notebook from the App Store. This was not the case, and after I read Professor O’Donnell’s Announcement, I was directed to the right software and how to access the right notebook. Now that I am working on the right notebook and interface, the tools and capabilities are very cool. I am also finding that Professor O’Donnell’s guides to work through all the proper settings and configurations are extremely helpful and comprehensive.\n\nGitHub\nGithub to me at first was very tedious, and I am still not 100% sure if this blog is going to upload correctly after I upload it to Github, Lol. what I know about GitHub from a brief online research is that GitHub is a cloud-based platform that allows developers to store, manage, and collaborate on code. GitHub is built on Git, a version control system created by Linus Torvalds.\n\nI am excited to explore how we will use GitHub throughout the semester.\n\n\nLLMs\nLLMs are Large Language Models that are trained to understand and generate human-like language. They are called “Large” because they are trained on massive datasets, they have Billions and sometimes even Trillions of parameters.\nWhat I have found particularly interesting, especially as it pertains to this class, is that these models are trained by reading huge amounts of text to learn patterns of language. Essentially, these models are predicting the next word in a sequence; the bigger the set, the more nuanced and refined it becomes, therefore making it more useful.\nLLMs can:\n\nGenerate text\nAnswer questions\nSummarization (turn long documents into short overviews)\nTranslation between languages\nProvide programming help (autocompleting or fixing code)\nConversational AI (chatbots, personal assistants)\n\nHowever, Limitations of LLMs can include: 1) LLMs don’t understand in the human sense 2) They predict patterns, not truth 3) They can generate hallucinations (confident but wrong answers) 4) Training requires massive computing power and energy 5) May carry biases from their training data"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Marco",
    "section": "",
    "text": "I am a junior here at Penn studying Economic History and Data Science with a strong interest in Finance and Entrepreneurship. I live in Fairfield, CT, but am originally from Vail, CO. Outside of the classroom, I am a member of the Varsity Men’s Lacrosse Team and Finance Club. These blogs are about my explorations with AIs. Additionally, I will write about recent AI news from time to time."
  }
]