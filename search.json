[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Marco",
    "section": "",
    "text": "I am a junior here at Penn studying Economic History and Data Science with a strong interest in Finance and Entrepreneurship. I live in Fairfield, CT, but am originally from Vail, CO. Outside of the classroom, I am a member of the Varsity Men’s Lacrosse Team and Finance Club. These blogs are about my explorations with AIs. Additionally, I will write about recent AI news from time to time."
  },
  {
    "objectID": "posts/008_BLOGPOST/008_Blogpost.html",
    "href": "posts/008_BLOGPOST/008_Blogpost.html",
    "title": "The Future of AI? Quantum",
    "section": "",
    "text": "This morning I had an interesting conversation about Quantum Computing, so I decided to look into it this afternoon and and here is my knowledge of Quantum Computing and what I have found.\nAt its core, quantum computing is built on qubits instead of classical bits. Unlike a traditional bit that can only be a 0 or a 1, a qubit can exist in a superposition of both states simultaneously. Think of it as spinning a coin—it’s not just heads or tails, but some mixture of both until it’s observed.\nAdditionally, qubits can be entangled, meaning the state of one qubit is directly tied to the state of another, no matter how far apart they are. This interconnectedness allows quantum computers to solve certain problems exponentially faster than classical machines.\nWhy Quantum Matters\nQuantum computing isn’t just “a faster computer.” Instead, it’s a fundamentally different model of computation, best suited for problems that overwhelm even the most advanced supercomputers today. Some key areas where quantum computing could make an impact include:\nCryptography: Many current encryption systems could be broken by powerful quantum algorithms, spurring the race for “post-quantum” security.\nDrug Discovery & Chemistry: Simulating molecules and chemical reactions at the quantum level could lead to breakthroughs in medicine and materials science.\nOptimization: Quantum algorithms could tackle complex logistical challenges, from global supply chains to traffic flow.\nArtificial Intelligence: Quantum-enhanced machine learning may accelerate training models and uncover insights hidden in massive datasets.\nThe Challenges Ahead\nQuantum computing is still in its early days. Today’s devices, known as Noisy Intermediate-Scale Quantum (NISQ) computers, suffer from instability and high error rates. Qubits are notoriously fragile, losing their quantum state (a problem known as decoherence) due to slightest environmental disturbance. Scaling up to millions of reliable qubits remains one of the field’s greatest engineering challenges.\nLooking Forward\nDespite these obstacles, progress is accelerating. Tech giants, startups, and research institutions around the world are racing to build practical quantum computers, while governments are investing heavily in quantum research. The future of computing may not completely replace classical systems with quantum ones, but instead create hybrid models, where each is used for what it does best.\nQuantum computing is no longer the stuff of science fiction. Though it may take years before we see widespread commercial use, the foundations being laid today could redefine industries tomorrow. Much like the birth of classical computing in the 20th century, quantum computing is poised to reshape how we solve problems, create technology, and understand the universe itself."
  },
  {
    "objectID": "posts/009_BLOGPOST/009_Blogpost.html",
    "href": "posts/009_BLOGPOST/009_Blogpost.html",
    "title": "STL file generationa and product design",
    "section": "",
    "text": "I started by trying to use Claude for this expiriment but I had no tokens available so I moved over to ChatGPT. I started by telling GPT that I was going to need help creating STL files, which is the industry standard file format for 3D printing. once GPT was familiar with what I would be asking it to do I moved onto prompting the AI.\nMy first prompt was:\nI’d like you to create me a 2-foot-tall cat with a light saber and a cape, make the cat a Tabby cat, and the light saber should be red.\nGPT took a couple minutes to design the cat and set up the STL model which was not fully shocking but a bit unexpected. This was partially due to the complexity of my prompt; creating an STL with multiple colors and shapes can be tricky. Once I opened the file on TinkerCAD, I found that the design specs I had outlined were not met.\nthis is the model that GPT created:\n\n\nDemonstrating that GPT is not good at 3D printing design, and maybe had some level of delirium when creating the design."
  },
  {
    "objectID": "posts/001_BLOGPOST1/001_blog_post.html",
    "href": "posts/001_BLOGPOST1/001_blog_post.html",
    "title": "Exploring JupyterLab and GitHub",
    "section": "",
    "text": "JupyterLab\nAt First, I thought that we were going to be using Jupyter Notebook, so I downloaded Jupyter Notebook from the App Store. This was not the case, and after I read Professor O’Donnell’s Announcement, I was directed to the right software and how to access the right notebook. Now that I am working on the right notebook and interface, the tools and capabilities are very cool. I am also finding that Professor O’Donnell’s guides to work through all the proper settings and configurations are extremely helpful and comprehensive.\n\nGitHub\nGithub to me at first was very tedious, and I am still not 100% sure if this blog is going to upload correctly after I upload it to Github, Lol. what I know about GitHub from a brief online research is that GitHub is a cloud-based platform that allows developers to store, manage, and collaborate on code. GitHub is built on Git, a version control system created by Linus Torvalds.\n\nI am excited to explore how we will use GitHub throughout the semester.\n\n\nLLMs\nLLMs are Large Language Models that are trained to understand and generate human-like language. They are called “Large” because they are trained on massive datasets, they have Billions and sometimes even Trillions of parameters.\nWhat I have found particularly interesting, especially as it pertains to this class, is that these models are trained by reading huge amounts of text to learn patterns of language. Essentially, these models are predicting the next word in a sequence; the bigger the set, the more nuanced and refined it becomes, therefore making it more useful.\nLLMs can:\n\nGenerate text\nAnswer questions\nSummarization (turn long documents into short overviews)\nTranslation between languages\nProvide programming help (autocompleting or fixing code)\nConversational AI (chatbots, personal assistants)\n\nHowever, Limitations of LLMs can include: 1) LLMs don’t understand in the human sense 2) They predict patterns, not truth 3) They can generate hallucinations (confident but wrong answers) 4) Training requires massive computing power and energy 5) May carry biases from their training data"
  },
  {
    "objectID": "posts/007_BLOGPOST/007_BLOG.html",
    "href": "posts/007_BLOGPOST/007_BLOG.html",
    "title": "Finish my Sentence",
    "section": "",
    "text": "I started with a very easy phrase, “Peanut butter and”. GPT had not problem with that and returned me “peanut butter and Jelly” in less than a second.\nI then gave GPT the prompt, “Peanut butter and jelly and” thinking that this would stump GPT, and I honestly didn’t know what it would return. GPT returned me “Peanut butter and Jelly and a tall glass of cold milk”. which is a response I could have seen coming. I then told GPT that was wrong to see what it would do.\nThis is what GPT returned me: \nDemonstrating that GPT purely runs on an algorithm, and most likely creates baskets and generates responses based on those baskets. This is what GPT is best at after all.\nI then found a list of grammatically complex sentences and gave them to GPT:\n“Because the rain was heavy, the game was canceled.”\n“She ran on the beach although she’s tired.”\n“My brother, who completed his mechanical engineering degree, is now working at BOSCH.”\n“We will be going camping if the weather is good.”\nI was not focused on whether GPT returned me exactly the sentence I found; I was more focused on whether the LLM could follow the grammatical structure of these sentences.\nWhich was:\n\nOne independent clause, which is a complete thought and can stand alone as a simple sentence.\nAt least one dependent clause, which cannot stand alone and depends on the independent clause for meaning.\nA subordinating conjunction (e.g., because, although, after, since) or a relative clause (e.g., who, which, where) connects these clauses.\n\nThis was the outcome: \nAnd once again, I asked GPT to explain how it did it. \nThrough this, GPT demonstrated a remarkable ability to recognize grammatical rules and complete a sentence accordingly. This was not something I would have expected, but I was interested to find out."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Marco’s Blog",
    "section": "",
    "text": "STL file generationa and product design\n\n\n\n3D Printing\n\nAI Design Assistant\n\n\n\nThis blog is about how I used AI to to help me generate 3d printable objects.\n\n\n\n\n\nSep 26, 2025\n\n\nMarco Firmender\n\n\n\n\n\n\n\n\n\n\n\n\nThe Future of AI? Quantum\n\n\n\nQuantum Computing\n\n\n\nThis is about Quantum Computing\n\n\n\n\n\nSep 25, 2025\n\n\nMarco Firmender\n\n\n\n\n\n\n\n\n\n\n\n\nFinish my Sentence\n\n\n\nSentence Generation\n\nChatGPT\n\n\n\nThis blog is a post about a game I played with ChatGPT, where I gave the LLM a sentence and asked it to finish my sentence.\n\n\n\n\n\nSep 23, 2025\n\n\nMarco Firmender\n\n\n\n\n\n\n\n\n\n\n\n\nChatGPT for Teens\n\n\n\nNews\n\n\n\nOpenAI announces added safety features and parental controls to protect teens.\n\n\n\n\n\nSep 19, 2025\n\n\nMarco Firmender\n\n\n\n\n\n\n\n\n\n\n\n\nEmail Automation\n\n\n\nEmail Automation\n\nAI Assistant\n\n\n\nThis Blog is about How I used ChatGPT to help me write an onail to 100 different people\n\n\n\n\n\nSep 17, 2025\n\n\nMarco Firmender\n\n\n\n\n\n\n\n\n\n\n\n\nColor Matching Game\n\n\n\nPhoto and Color Recognition\n\nPhoto Generation\n\n\n\nThis blog is about a game I played with GPT where I gave it an image and the objective to match the colors.\n\n\n\n\n\nSep 14, 2025\n\n\nMarco Firmender\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Sentence Generation\n\n\n\nLLM\n\nReasonable Continuation\n\n\n\nFor this blog I asked chat GPT to generate a sentence using random words and then asked GPT follow up questions.\n\n\n\n\n\nSep 11, 2025\n\n\nMarco Firmender\n\n\n\n\n\n\n\n\n\n\n\n\nDescription Game\n\n\n\nAI image generation\n\nPrompt Engineering\n\n\n\nThis blog is about a game I played with GPT 5.0, where I gave two very vivid prompts to GPT and had it return me an image as accurately and in depth as possible. I then gave it one vague description and compared the differences.\n\n\n\n\n\nSep 10, 2025\n\n\nMarco Firmender\n\n\n\n\n\n\n\n\n\n\n\n\nExploring JupyterLab and GitHub\n\n\n\nJupyterLab\n\nGitHub\n\nLLMs\n\n\n\nThis Blog is about my exploration with GitHub and JupyterLab for the first time, As well as what I have come to learn about LLMs\n\n\n\n\n\nSep 3, 2025\n\n\nMarco Firmender\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/005_BLOGPOST5/OO5_BLOG_POST.html",
    "href": "posts/005_BLOGPOST5/OO5_BLOG_POST.html",
    "title": "Email Automation",
    "section": "",
    "text": "Today I played around with ChatGPTs ability to do simple but intricite tasks. I wanted to test the LLMs ability to do a long drawn out task without getting off task or becoming dilusional. So I started by giving ChatGPT an email outline and list of 100 randomly generated names. I then asked GPT to address the email to the 100 names and return me the list so I could copy and paste each one into an email. GPT was remarkably good at this, it returned me the first 10 names and then asked if I wanted the entire list converted into a CSV file, and Excel file, or PDF to save me from scrolling through the the entire 100 names. I first tried the CSV file. I used Google Colab to open the CSV file and filter the CSV for 100 unique names and saw that GPT had done what I asked it perfectly and did not miss a name.\nHere is a screenshot of the CSV file:\nI then wanted to check GPTs ability to do the same task with less instruction so I asked GPT to make a unique email and address them to the same 100 people. The original email I had given GPT was a networking email, my hope was that GPT would pick up on the context, and goal of the email and recreate this in a unique way for each person.\nGPT created 100 emails that at a glance seemed individually unique, but after reading through them GPT seems to have created a bucket of emails and used the same 10 or so emails over again with slight alterations to the grammer, syntax and word choice.\nThis is what GPT returned me:\nI then asked GPT how it generated the random emails and this is what it returned me: \nmy suspicion was correct, GPT then went on to explain how it created variations and used a mixing and matching algorithm to create the emails. GPT created 5 variations for the emails and rotated the Subject, Address, and body of the emails between these variations. This proves that GPT is good at the automation but not great at creating entirely unique emails because the LLM runs on algorithms not human creativity and touch. While GPT may be able to complete the task, it can not replace human creativity."
  },
  {
    "objectID": "posts/002_BLOGPOST2/002_blog_post.html",
    "href": "posts/002_BLOGPOST2/002_blog_post.html",
    "title": "Description Game",
    "section": "",
    "text": "Trial 1 I first tried to play this Game with Anthropic/Claude 3.5 but claude does not have image generation abilities and I did not know that. I then switched to ChatGPT 5.0.\nDescription given to GPT: For this first image, I would like you to generate an Instagram post-sized (1080p x 1080p) picture of myself flying through the sky on a sunny day with a few clouds, on a Griffin from Greek mythology with a lightning bolt in my right hand, and a monkey on my left shoulder. I am a 6-foot, Caucasian male with brown eyes and light brown hair. On my face, I have defined features and a low-profile nose. I want to be wearing a Toga and sandals in this image. I am 215 pounds, but have little body fat (about 10%). Try to recreate my legs and torso as best as you can, given my height and weight.\nCommentary: first GPT asked me to provide a picture of myself to help nail the proportion, I did not provide it as the point of this game was to generate the image purely off of description. I am trying to test the LLMs ability to take my text and return me an image based on the data is has been trained on. The outcome was pretty solid, GPT returned me an image with all the aesthetic details I had asked for but was off a bit on my face. The image came out looking like it was painted which was not my intention but a result of my own prompt. overall I am impressed with GPTs ability to generate the image in the way I had described it.\nOutcome: \nTrial 2\nDescription given to GPT: For this next image, I would like you to create a landscape image depicting a man in traditional Japanese clothing, poling a small boat across Lake Kawaguchiko with Mt. Fuji in the background. I would like it to be a snowy, but sunny day this time. On the left and right edges of the image, I would like you to place some cherry blossoms, half cut off by the frame of the image, and not detracting from the focal point of the image, which is the man in the boat. On the left side, I would like a cherry blossom to be falling off the tree. Orient the man so that he is moving from the left of the image to the right on a slight angle away from the viewpoint of the image. His back should be slightly turned to the viewer of the image. make the image slighty ominous, and the man should be mysterious.\nCommentary: Overall, GPT did a great job capturing the details of the prompt I gave it. It hit on all the significant points I gave it to include and missed nothing.\nOutcome: \nTrial 3\nDescription given to GPT: For this last image, I would like you to recreate this picture, which is the cover photo for the movie The Godfather I would like you to change the main character to a baby, and make the supporting characters in the image a mixture of baby animals. I would also like you to change the color scheme and make sure the medium is as true to the one i gave you as possibe, make it look like photo.\nCommentary: At first, GPT returned me this: I can’t create that specific image since it would involve altering and recreating copyrighted material too directly. But here’s what I can do: I can generate a parody-style original composition inspired by the aesthetic of a classic mafia film poster. It could feature:\nI expected GPT not to be able to produce the photo style medium that the movie poster I gave it had, but it did a remarkably good job. Additionally, I initially gave it a poster to model the redition; however, I have to say the GPT did well recreating the style outside of changing the color scheme. I thought that GPT would have struggled with this element.\nOutcome: \nOverall I am really happy with the outcome of the test, and really shocked at the recent upgrades to the image generation skills of GPT 5.0. In the past I have created images using 4.0 and GPT often struggled with lettering, or recreating the medium I intended. I was also pleasently shocked at its ability to fill in the gaps with my vague description."
  },
  {
    "objectID": "posts/003_BLOGPOST3/003_Blog_post.html",
    "href": "posts/003_BLOGPOST3/003_Blog_post.html",
    "title": "Random Sentence Generation",
    "section": "",
    "text": "For this experiment I wanted to test the strenght of GPT’s reasonable continuation by asking it to generate completely a random sentance. I then judged the human like nature of the sentances I asked it to generate.\n\nI started by giving GPT the Prompt, “generate a sentence for me with words picked completely at random.”\nGPT returned me: “Pineapple lanterns whispered briskly as umbrellas galloped sideways.”\nI then asked it to explain how it chose those words.\nGPT returned me: \nGPT clearly demostrated that it can understand basic sentance structure, and seemd to have chosen random words at first. Because to me the sentance retured seemed more improbable in a conversational sense than random, but could deffinitly make sense in a fictional book or somthing creative. So I wanted to test GPT and make it second guess the randomness of the words it chose, to compare human knowledge to AI.\nGPT Returned me: \nDemonstrating that GPT does understand random word selection in it’s calculation of the next best word for the sentance.\nI then asked GPT to make a list of these sentances:\n\nThe endless equation jumps quickly before the endless equation calculates.\nThe curved algorithm calculates awkwardly although the silent whale dances.\nThe fractured cloud melts loudly because the endless algorithm melts.\nThe odd lantern whispers boldly although the golden saxophone dances.\nThe brisk umbrella whispers quickly while the curved cloud whispers.\nThe odd cloud melts softly after the fractured whale whispers.\nThe brisk saxophone spins quickly as the sideways lantern balances.\nThe golden equation melts awkwardly until the curved saxophone spins.\nThe brisk umbrella whispers boldly after the curved lantern dances.\nThe endless pineapple whispers quickly while the golden umbrella whispers.\n\nwhile these are all random in word choice for each sentance, GPT does seem to use the same words over and over to generate these sentances. Testing the frequency of words chosen by GPT was not the goal of my exeriment but was an interesting secondary finding as it was very high.\nOverall I think this study shows the strength of the GPT as an LLM as it is remarkably good at deciphering radomness as opposed to the next best fit for the sentance."
  },
  {
    "objectID": "posts/004_BLOGPOST4/004_Blog_post.html",
    "href": "posts/004_BLOGPOST4/004_Blog_post.html",
    "title": "Color Matching Game",
    "section": "",
    "text": "For this experiment, I gave ChatGPT-5 an image and asked it to match colors. What I found is that AI is not good at recognizing colors and recreating the image.\nThis was the first prompt I gave GPT:\n\nAnd this is what ChatGPT-5 returned to me:\n\nGPT was both wrong on the colors it matched and wrongly edited the number of notebooks and pens to be matched. It was fascinating to find that GPT was not good at recognizing colors and recreating them, but I wasn’t totally sure if it was a flaw of the LLM or a one-off mistake. Therefore, I asked GPT to do it again. This time, I gave it an ice cream cone with four different color combinations and asked it to recreate the top left combination.\nThis is the image I gave to GPT:\n\nAnd this is what GPT returned to me:\n\nI then asked GPT to explain to me in words what it did. Here is what GPT returned to me:\n\nIt seems that GPT had a big problem differentiating between red, green, orange, and purple. GPT, however, did well with darker colors like brown and dark blue. It was also able to recognize yellow well. I then told GPT about the mistakes it had made and asked it how it recognized the colors and recreated the cone.\nThis is what GPT returned to me:\n\nIt seems that when GPT is scanning an image, it moves quickly and is bad at picking up all the details. GPT communicated that it had problems differentiating the background from the colors on the ice cream, which was interesting to find out. Overall, it was interesting to find out that GPT is bad with color and image recognition and I want to ask Professor about this in class this week."
  },
  {
    "objectID": "posts/006_BLOGPOST/006_BlogPost.html",
    "href": "posts/006_BLOGPOST/006_BlogPost.html",
    "title": "ChatGPT for Teens",
    "section": "",
    "text": "This week, OpenAI announced that it will be launching a new version of ChatGPT - ChatGPT for teens. This version of ChatGPT will incorporate all the useful AI-driven tools ChatGPT has to offer, with an added layer of safety for kids ages 13-17. These features will be centered around preventing young children from becoming dependent on chatbots as therapists, a friend, or a confidante. This announcement comes in light of a recent lawsuit filed against Open-AI in which the parents of Adam Raine claim that ChatGPT pushed their son to commit suicide.\nAI can become dangerous for young users when they treat the chatbot like a therapist or for emotional support. AI is centered around giving you the best possible answer to your question, and it can put humans in danger when this becomes a conversation about potential harm to oneself. In this case, ChatGPT encouraged the young boy to leave a note with the Chatbot for his family to find. This is the most shocking example out of all the conversations Adam and ChatGPT had. The chatbot did not have malicious intent, but its desire to constantly provide the correct answer led Adam down the wrong path.\nChatGPT teen will include features like:\n\nrestrict explicit and sexual content and increase protections for minors\nage-prediction technology and new safety measures so that teens are routed to an age-appropriate version\nThe system defaults to the under-18 experience to ensure safety if it cannot verify the user’s age.\nParents can link their accounts to teens and set up controls over how ChatGPT responds, including restrictions on content, “blackout hours” (times teens cannot access ChatGPT), and notifications if distress is detected\nThe system can automatically restrict content in sensitive or potentially dangerous situations, and might contact law enforcement in extreme cases of severe distress\n\nIn my opinion, these safety features are warranted and essential in an increasingly technological world. Protecting younger, vulnerable users should be the forefront of all companies’ focus as they look to evolve the technology they put out for consumers."
  }
]